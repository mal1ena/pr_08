# pr_08
## Цель: ##
Определите наиболее эффективный метод загрузки данных (малых и больших объёмов) из CSV-файлов в СУБД PostgreSQL, сравнив время выполнения методов: pandas.to_sql(), psycopg2.copy_expert() (с файлом и с io.StringIO) и пакетная вставка (psycopg2.extras.execute_values).

## Структура репозитория: ##
- `lab_08.ipynb` — SQL скрипт для создания базы данных и таблиц, анализ методов
- `` —  выполнение всех индивидуальных заданий.


## Задачи: ##
Подключиться к предоставленной базе данных PostgreSQL.
Проанализировать структуру исходных CSV-файлов (upload_test_data.csv, upload_test_data_big.csv).
Реализовать три различных метода загрузки данных в PostgreSQL(pandas.to_sql(), copy_expert(), io.StringIO).
Измерьте время, затраченное каждым методом на загрузку данных из небольшого файла (upload_test_data.csv).
Измерьте время, затраченное каждым методом на загрузку данных из большого файла (upload_test_data_big.csv).
Визуализировать результаты сравнения времени загрузки с помощью гистограммы (matplotlib).
Сделать выводы об эффективности каждого метода для разных объемов данных.
Выполнить индивидуальные задания

## Выполнение: ##
1. Подключаксчя к базе данных

![image](https://github.com/user-attachments/assets/71af8924-9c51-4f66-9496-8c7ce6c14e72)

2. Анализируем структуру исходных CSV-файлов (upload_test_data.csv, upload_test_data_big.csv)

![image](https://github.com/user-attachments/assets/ff0860d2-ac6f-485d-87bb-c11098ff190c)

3. Реализуем три различных метода загрузки данных в PostgreSQL(pandas.to_sql(), copy_expert(), io.StringIO).Измеряем время, затраченное каждым методом на загрузку данных из небольшого файла (upload_test_data.csv) и из большого файла (upload_test_data_big.csv).

![image](https://github.com/user-attachments/assets/e8202a42-13f0-434a-8892-2a555a686ba4)

4.Визуализируем результаты сравнения времени загрузки с помощью гистограммы (matplotlib).

![image](https://github.com/user-attachments/assets/a89f1538-efe3-453e-b341-51ee81c42bf7)

5. Анализ результатов:

**Малый файл (`upload_test_data.csv`):**
*   На малых объемах данных (1000 строк) разница во времени выполнения между методами может быть не столь значительной, хотя `COPY` (`copy_expert`) часто показывает лучшие результаты из-за минимальных накладных расходов.
*   `pandas.to_sql` (с `method='multi'`) худшая производительность, и обычно уступают `COPY`. Метод `copy_expert` с `StringIO` добавляет небольшие накладные расходы на конвертацию DataFrame -> CSV в памяти по сравнению с прямым `COPY` из файла.

**Большой файл (`upload_test_data_big.csv`):**
*   На больших объемах данных (~900 тыс. строк) разница становится **критически важной**.
*   **`copy_expert (file)`** почти всегда является **бесспорным лидером**. Команда `COPY` в PostgreSQL чрезвычайно оптимизирована для массовой загрузки непосредственно из файла (или потока STDIN). Она минимизирует транзакционные издержки и накладные расходы на уровне SQL-парсера.
*   **`copy_expert (StringIO)`** также очень быстр, но немного уступает файловой версии из-за необходимости сначала создать CSV-строку в памяти. Если данные уже находятся в DataFrame в памяти, это может быть хорошим вариантом, но если исходник - файл, прямой `COPY` из файла эффективнее. *Примечание: Если большой файл не помещается в память Colab для создания DataFrame, этот метод не сработает или его результаты будут неточными.*
*   **`pandas.to_sql`** (даже с `chunksize` и `method='multi'`) значительно медленнее, чем `COPY`. Он имеет больше накладных расходов на уровне Python/SQLAlchemy и формирование SQL-запросов, хотя использование `chunksize` предотвращает проблемы с памятью.

6. Индивидуальные задания
Создать таблицы sales_small, sales_big.

![image](https://github.com/user-attachments/assets/66e49fed-224b-4f29-82d8-61020140c063)

Использовать метод: pandas.to_sql() для загрузки малых данных (sales_small)

![image](https://github.com/user-attachments/assets/f9964519-e082-43bb-b38d-4d39842c9602)

Использовать метод: copy_expert (file) для загрузки больших данных (sales_big)

![image](https://github.com/user-attachments/assets/bb2a69a2-8a99-48b8-9a68-4597ab1460f8)

Вычислить суммарное значение quantity в sales_small, где cost > 5,00.

![image](https://github.com/user-attachments/assets/ed4c592c-a5f6-4941-8977-56df3b88821c)

Построить столбчатую диаграмму топ-5 total_revenue из sales_small.

![image](https://github.com/user-attachments/assets/e9e771aa-f024-4c50-94f7-74000c7e2e2b)


## Вывод: ##
Для **максимальной производительности** при загрузке больших объемов данных из файлов в PostgreSQL **однозначно рекомендуется использовать команду `COPY`**, доступную через `psycopg2.copy_expert()` при работе с файлами напрямую.
Если данные уже находятся в памяти (например, в Pandas DataFrame) и их нужно загрузить, `psycopg2.copy_expert()` с `io.StringIO` является вторым по скорости вариантом, значительно опережая `pandas.to_sql` и `Batch Insert`.
 Методы `pandas.to_sql` могут быть удобны для небольших объемов данных или когда требуется большая гибкость в обработке данных перед вставкой в рамках Python-кода, но он существенно уступает `COPY` в производительности при работе с большими CSV-файлами. Важно также учитывать ограничения по памяти при использовании метода, требующего загрузки всего файла в DataFrame.



